{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.io as spio\n",
    "from matplotlib import pyplot as plt\n",
    "from imageio import imread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/udaysawhney/Desktop/create_dataset/dataset/\n"
     ]
    }
   ],
   "source": [
    "# define base paths for pascal the original VOC dataset training images\n",
    "base_dataset_dir_voc = '/Users/udaysawhney/Desktop/create_dataset/'\n",
    "images_folder_name_voc = \"dataset/\"\n",
    "annotations_folder_name_voc = \"SegmentationClass/\"\n",
    "images_dir_voc = os.path.join(base_dataset_dir_voc, images_folder_name_voc)\n",
    "print(images_dir_voc)\n",
    "annotations_dir_voc = os.path.join(base_dataset_dir_voc, annotations_folder_name_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_list(base_dataset_dir, images_folder_name, annotations_folder_name, filename):\n",
    "    images_dir = os.path.join(base_dataset_dir, images_folder_name)\n",
    "    annotations_dir = os.path.join(base_dataset_dir, annotations_folder_name)\n",
    "\n",
    "    file = open(filename, 'r')\n",
    "    images_filename_list = [line for line in file]\n",
    "    return images_filename_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of training images: 32\n"
     ]
    }
   ],
   "source": [
    "test_images_filename_list = get_files_list(base_dataset_dir_voc, images_folder_name_voc, annotations_folder_name_voc, \"/Users/udaysawhney/Desktop/create_dataset/custom_test.txt\")\n",
    "print(\"Total number of training images:\", len(test_images_filename_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATASET_DIR=\"./tfrecords/\"\n",
    "if not os.path.exists(TRAIN_DATASET_DIR):\n",
    "    os.mkdir(TRAIN_DATASET_DIR)\n",
    "    \n",
    "TEST_FILE = 'test123.tfrecords'\n",
    "test_writer = tf.python_io.TFRecordWriter(os.path.join(TRAIN_DATASET_DIR,TEST_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_annotation_from_xml_file(annotations_dir, image_name):\n",
    "    annotations_path = os.path.join(annotations_dir, (image_name.strip() + \".xml\"))\n",
    "    print(annotations_path)\n",
    "    xml = spio.loadmat(annotations_path)\n",
    "    \n",
    "    print('HERE NIGA')\n",
    "    img = xml['GTcls']['Segmentation'][0][0]\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, image_name in enumerate(test_images_filename_list):\n",
    "    image_np = imread(os.path.join(images_dir_voc, image_name.strip() + \".jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfrecord_dataset(filename_list, writer):\n",
    "\n",
    "    # create training tfrecord\n",
    "    read_imgs_counter = 0\n",
    "    for i, image_name in enumerate(filename_list):\n",
    "        image_name = str(image_name)\n",
    "\n",
    "        try:\n",
    "            image_np = imread(os.path.join(images_dir_voc, image_name.strip() + \".jpg\"))\n",
    "        except FileNotFoundError:\n",
    "            try:\n",
    "                # read from Pascal VOC path\n",
    "                image_np = imread(os.path.join(images_dir_voc, image_name.strip() + \".jpg\"))\n",
    "            except FileNotFoundError:\n",
    "                print(os.path.join(images_dir_voc, image_name.strip() + \".jpg\"))\n",
    "                print(\"File:\",image_name.strip(),\"not found.\")\n",
    "                continue\n",
    "        try:\n",
    "            print('FUCK TAHT')\n",
    "            annotation_np = read_annotation_from_xml_file(annotations_dir_voc, image_name)\n",
    "        except FileNotFoundError:\n",
    "            # read from Pascal VOC path\n",
    "            try:\n",
    "                annotation_np = imread(os.path.join(annotations_dir_voc, image_name.strip() + \".png\"))\n",
    "                print('SUCCESS!!')\n",
    "            except FileNotFoundError:\n",
    "                print(\"File:\",image_name.strip(),\"not found lolololol.\")\n",
    "                continue\n",
    "            \n",
    "        read_imgs_counter += 1\n",
    "        image_h = image_np.shape[0]\n",
    "        image_w = image_np.shape[1]\n",
    "\n",
    "        img_raw = image_np.tostring()\n",
    "\n",
    "        example = tf.train.Example(features=tf.train.Features(feature={\n",
    "                'height': _int64_feature(image_h),\n",
    "                'width': _int64_feature(image_w),\n",
    "                'image_raw': _bytes_feature(img_raw),\n",
    "        }))\n",
    "\n",
    "        writer.write(example.SerializeToString())\n",
    "    \n",
    "    print(\"End of TfRecord. Total of image written:\", read_imgs_counter)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FUCK TAHT\n",
      "/Users/udaysawhney/Desktop/create_dataset/SegmentationClass/0000_nir.xml\n",
      "File: 0000_nir not found lolololol.\n",
      "FUCK TAHT\n",
      "/Users/udaysawhney/Desktop/create_dataset/SegmentationClass/0005_nir.xml\n",
      "File: 0005_nir not found lolololol.\n",
      "FUCK TAHT\n",
      "/Users/udaysawhney/Desktop/create_dataset/SegmentationClass/0006_nir.xml\n",
      "File: 0006_nir not found lolololol.\n",
      "FUCK TAHT\n",
      "/Users/udaysawhney/Desktop/create_dataset/SegmentationClass/0007_nir.xml\n",
      "File: 0007_nir not found lolololol.\n",
      "FUCK TAHT\n",
      "/Users/udaysawhney/Desktop/create_dataset/SegmentationClass/0008_nir.xml\n",
      "File: 0008_nir not found lolololol.\n",
      "FUCK TAHT\n",
      "/Users/udaysawhney/Desktop/create_dataset/SegmentationClass/0017_nir.xml\n",
      "File: 0017_nir not found lolololol.\n",
      "FUCK TAHT\n",
      "/Users/udaysawhney/Desktop/create_dataset/SegmentationClass/0020_nir.xml\n",
      "File: 0020_nir not found lolololol.\n",
      "FUCK TAHT\n",
      "/Users/udaysawhney/Desktop/create_dataset/SegmentationClass/0025_nir.xml\n",
      "File: 0025_nir not found lolololol.\n",
      "FUCK TAHT\n",
      "/Users/udaysawhney/Desktop/create_dataset/SegmentationClass/0026_nir.xml\n",
      "File: 0026_nir not found lolololol.\n",
      "FUCK TAHT\n",
      "/Users/udaysawhney/Desktop/create_dataset/SegmentationClass/0027_nir.xml\n",
      "File: 0027_nir not found lolololol.\n",
      "FUCK TAHT\n",
      "/Users/udaysawhney/Desktop/create_dataset/SegmentationClass/0033_nir.xml\n",
      "File: 0033_nir not found lolololol.\n",
      "FUCK TAHT\n",
      "/Users/udaysawhney/Desktop/create_dataset/SegmentationClass/0035_nir.xml\n",
      "File: 0035_nir not found lolololol.\n",
      "FUCK TAHT\n",
      "/Users/udaysawhney/Desktop/create_dataset/SegmentationClass/0037_nir.xml\n",
      "File: 0037_nir not found lolololol.\n",
      "FUCK TAHT\n",
      "/Users/udaysawhney/Desktop/create_dataset/SegmentationClass/0039_nir.xml\n",
      "File: 0039_nir not found lolololol.\n",
      "FUCK TAHT\n",
      "/Users/udaysawhney/Desktop/create_dataset/SegmentationClass/0040_nir.xml\n",
      "File: 0040_nir not found lolololol.\n",
      "FUCK TAHT\n",
      "/Users/udaysawhney/Desktop/create_dataset/SegmentationClass/0045_nir.xml\n",
      "File: 0045_nir not found lolololol.\n",
      "FUCK TAHT\n",
      "/Users/udaysawhney/Desktop/create_dataset/SegmentationClass/0047_nir.xml\n",
      "File: 0047_nir not found lolololol.\n",
      "FUCK TAHT\n",
      "/Users/udaysawhney/Desktop/create_dataset/SegmentationClass/0048_nir.xml\n",
      "File: 0048_nir not found lolololol.\n",
      "FUCK TAHT\n",
      "/Users/udaysawhney/Desktop/create_dataset/SegmentationClass/0049_nir.xml\n",
      "File: 0049_nir not found lolololol.\n",
      "FUCK TAHT\n",
      "/Users/udaysawhney/Desktop/create_dataset/SegmentationClass/0050_nir.xml\n",
      "File: 0050_nir not found lolololol.\n",
      "FUCK TAHT\n",
      "/Users/udaysawhney/Desktop/create_dataset/SegmentationClass/0051_nir.xml\n",
      "File: 0051_nir not found lolololol.\n",
      "FUCK TAHT\n",
      "/Users/udaysawhney/Desktop/create_dataset/SegmentationClass/0053_nir.xml\n",
      "File: 0053_nir not found lolololol.\n",
      "FUCK TAHT\n",
      "/Users/udaysawhney/Desktop/create_dataset/SegmentationClass/0064_nir.xml\n",
      "File: 0064_nir not found lolololol.\n",
      "FUCK TAHT\n",
      "/Users/udaysawhney/Desktop/create_dataset/SegmentationClass/0069_nir.xml\n",
      "File: 0069_nir not found lolololol.\n",
      "FUCK TAHT\n",
      "/Users/udaysawhney/Desktop/create_dataset/SegmentationClass/0075_nir.xml\n",
      "File: 0075_nir not found lolololol.\n",
      "FUCK TAHT\n",
      "/Users/udaysawhney/Desktop/create_dataset/SegmentationClass/0076_nir.xml\n",
      "File: 0076_nir not found lolololol.\n",
      "FUCK TAHT\n",
      "/Users/udaysawhney/Desktop/create_dataset/SegmentationClass/0079_nir.xml\n",
      "File: 0079_nir not found lolololol.\n",
      "FUCK TAHT\n",
      "/Users/udaysawhney/Desktop/create_dataset/SegmentationClass/0084_nir.xml\n",
      "File: 0084_nir not found lolololol.\n",
      "FUCK TAHT\n",
      "/Users/udaysawhney/Desktop/create_dataset/SegmentationClass/0087_nir.xml\n",
      "File: 0087_nir not found lolololol.\n",
      "FUCK TAHT\n",
      "/Users/udaysawhney/Desktop/create_dataset/SegmentationClass/0094_nir.xml\n",
      "File: 0094_nir not found lolololol.\n",
      "FUCK TAHT\n",
      "/Users/udaysawhney/Desktop/create_dataset/SegmentationClass/0096_nir.xml\n",
      "File: 0096_nir not found lolololol.\n",
      "FUCK TAHT\n",
      "/Users/udaysawhney/Desktop/create_dataset/SegmentationClass/0097_nir.xml\n",
      "File: 0097_nir not found lolololol.\n",
      "End of TfRecord. Total of image written: 0\n"
     ]
    }
   ],
   "source": [
    "create_tfrecord_dataset(test_images_filename_list, test_writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
